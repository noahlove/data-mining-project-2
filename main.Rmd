---
title: "Project 2 - Job Hunt"
author: "Noah Love and Ido Li On"
date: "3/4/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, message=FALSE, warning=FALSE, include=FALSE}

library(tidyverse)
library(tidyjson)
library(jsonlite)
library(text2vec)
library(data.table)
```
Code for the project can be found [here]("https://github.com/noahlove/data-mining-project-2.git")

## Introduction

The data provided to us for this report is a scraped job postings from [indeed.com](https://www.indeed.com/ "https://www.indeed.com/"). They were scraped from September 2020 to March 2021, in four distinct scrapes.

The data is presented in a json file. It is a list of 16 - 25 (depending on the scrape) lists, each representing a unique job category: i.e. ux designer, recruiter or marketing. Within each of the 16 lists, are 2 lists: the first is a list of 4, describing position, location, job time (full or part time), and start time. The second list contains job descriptions associated with that job category.

Here is an (shortened) example of how each job postings appears in JSON:

> "2ccc2596d13c54d4": "Job to be done\nWe\u2019re the best way to get better.\n\nAbout Buoy Health:\nBuoy builds a digital health tool that helps people \u2013 from the moment they get sick \u2013 start their health care on the right foot. Started by a team of doctors and computer scientists working at the Harvard Innovation Laboratory in Boston MA, Buoy was developed in direct response to the downward spiral we\u2019ve all faced when we attempt to self-diagnose our symptoms online...."

### Clean up the data

Our first major choice was to combine all four data sets (different dates) into one large csv. None of our questions necessarily related to the time aspect of the data so we chose to just unify them. Here is an example of the list of categories, and associated characteristics.

```{r echo=FALSE}
full_df <- as_tibble(read.csv("Indeed_data/merged.csv"))

job_positions <- full_df %>% 
  group_by(job_title) %>% 
  summarise(n = n())

job_positions

```

First, there are only 25 categories here. Definitely not a comprehensive view of the job market, so if you are really set about going into a field that is not listed, this loop project explained below would not be a good fit. However, for most people majoring in statistics or computer science, this might provide a nice survey of various jobs. 

Another question that would be reasonable to ask is what is the distribution of job postings per category?

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data = job_positions, aes(x = job_title, y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  ggtitle("Distribution of the jobs by category for 2020") +
  coord_flip() +
  theme_light()+
  labs(x = "Job Title", y = "Number of jobs in the category")

```

Hopefully you don't want to be a data journalist because the data is not as comprehensive than the rest. But otherwise, it seems to be very equal across the categories. Otherwise, for the categories mentioned, it seems that the data is representative and could be very useful in a job hunt if you have interests in the related field. 

```{r echo=FALSE}
full_df %>% 
  group_by(job_location) %>% 
  summarise(n = n()) %>% 
  ggplot(aes(x = job_location, y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  ggtitle("Where are the jobs?") +
  labs(x = "State", y = "Number of jobs")
```
The data is likely most lacking in location. For people that were hoping for rural jobs, or maybe remote jobs, the data scraped is specifically focused on California and New York State which is not for everyone. Also, the data is particularly focused on mainly full time jobs. While there are percentages of other types of jobs including contract, internships and part-time, they are no where near as prevalent as full time jobs. 

```{r echo=FALSE}

internship <- sum(as.integer(as.logical(full_df$is_internship)))/length(full_df$is_fulltime)*100
parttime <- sum(as.integer(as.logical(full_df$is_parttime)))/length(full_df$is_fulltime)*100
contract <- sum(as.integer(as.logical(full_df$is_contract)))/length(full_df$is_fulltime)*100

special_jobs <- tibble("Percentage of jobs" = c(internship, parttime, contract), "Type of job"= c("Internships", "Part Time Jobs", "Contract Jobs"))

ggplot(data = special_jobs, aes(x = `Type of job`, y = `Percentage of jobs`)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Percentage of special job types")
  
  
```


## Wrangle the data

We wrangled all of these into a csv format for easier working. The data frame gives job location, salary minimum and maximum (rare), separate columns for different types of job positions (full time, part time, contract or internship) as well as job title (category), job_id (distinct id number from Indeed) and the job description from the company.

```{r message=FALSE, warning=FALSE, include=FALSE}
glimpse(full_df)
```



### Data Quality Issue

Some companies appear to have gotten lazy in their copy and pasting between websites and have smashed some words together. Because it is a rare enough occurrence, it will hopefully not have a large effect on the data. 

## Data processing

The data was then tokenized using the package text2vec, as well as broken into n-grams. Once the job descriptions were tokenized, then we could proceed to create a cosine similarity matrix for the job postings, identified by their Indeed ID. Cosine similiarity is a measure of similarity between two vectors, in this case two job postings. The values range from 0 to 1 (the same posting). This method was chosen because of the type of data. Instead of having to come up with a clever data normalization technique due to some companies being incredibly verbose while others are super brief, this takes care of that instead. Below is an example of the top left corner of the cosine matrix

```{r echo=FALSE, cache=FALSE}
cosine_matrix <- as_tibble(read.csv("Indeed_data/cos_dissm_mat.csv.gz"))

cosine_matrix[1:4, 1:4]
```

## User Input
Now, we will ask for user input, in a somewhat manual way. Based on your resume, we choose keywords from previous job positions, courses and trainings that you think would be relevant to your job search. 

How does this help? We will turn the words that are entered from your resume into a unique tokenized string as well. This will be put into the cosine similarity matrix, and we can compare which job postings you have the most in common with. Using the post ID, we can then filter the job postings such that you can get 3 recommendations from 3 distinct Indeed job categories to give you a variety of options to choose from.

```{r}
noah_input <- "Coins Gold Bullion Banking Diamonds Website Development Software Coding Programming Hedge Investment Fund Social Media Student Salesman Entrepreneurship, Startup, Incubator, HTML, R, Python"

ido_input <-  "Programming Cybersecurity Networks Reverse Engineering"
```

Here, Ido and Noah both provided key words from past education, job descriptions and titles that were in our resume. For the sake of testing both extremes, we have a more verbose input and a more concise one. 


```{r include=FALSE}
# Add user to csv, should need "description" and "job id"
user_full <- full_df %>%
  add_row(job_id = "RESUME (NOAH)", job_desc = noah_input, job_location = "A") %>%
  add_row(job_id = "RESUME (IDO)", job_desc = ido_input, job_location = "A") %>%
  arrange(job_location)

# head(user_full, 4) # make sure user was added
```

Here is a list of the most popular words use in job descriptions.

```{r include=FALSE}
it <- itoken(
  user_full$job_desc, # where to tokenize
  preprocessor = tolower, # How to preprocess
  tokenizer = word_tokenizer, # what tokenizer
  ids = user_full$job_id, # keep id as identifier
  progressbar = FALSE
)

stop_words <- c("a","an","and","are","as","at","be","by","for","from","have","in","is","i","not","of","on","or","other","our","ours","that","the","their","this","to","we","who","with","you","your","yours","me","my","myself","ourselves","")

vocab <- create_vocabulary(
  it,
  stopwords = stop_words,
  ngram = c(1L, 3L)
)


```
```{r echo=FALSE}
tail(vocab, 5)
```

```{r include=FALSE}
vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it, vectorizer)
# dim(dtm)
```

```{r include=FALSE}
tfidf <- TfIdf$new()
tfidf_dtm <- fit_transform(dtm, tfidf)

cossim <- as.data.frame(as.matrix(sim2(
  x = tfidf_dtm,
  method = "cosine",
  norm = "l2"
)))
```

Here is an example of the cosine similarity matrix. Noah's resume is the same as itself so it has a similarity of 1. It is also interestingly almost nothing like Ido's (very close to 0). In such a way, the two user inputs actually serve as a great example of how different strings show as very dissimilar and wouldn't be recommended. Ironically, we worked great as partners so maybe our user input isn't completed representative of us. 

In this way, the similarity score provides a metric for how similar or "good" the match is to our resume and provided terms. 


```{r echo=FALSE}
cossim_noah <- cossim %>% select(1)
cossim_ido <- cossim %>% select(2)

head(cossim_noah, 10)
```

As our way of creating diversity, we chose to separate by top posting per job category. If you are using a service like entering keywords to find a job, it is unlikely you know the exact field or even category of job you want. So to ensure a diverse option, we first outputted the most similar job posting to your resume in each job category. For example, the first 10:



```{r echo=FALSE}
prune_jobs_with_identical_sentences <- function(data, word_count = 8)
{
    result <- data

    while (1)
    {
        xit <- itoken(
          result$job_desc, # where to tokenize
          preprocessor = tolower, # How to preprocess
          tokenizer = word_tokenizer, # what tokenizer
          ids = result$job_id, # keep id as identifier
          progressbar = FALSE
        )

        xvocab <- create_vocabulary(xit, ngram = c(word_count,word_count))

        xvocab <- prune_vocabulary(xvocab, doc_count_min = 2)
        if (nrow(xvocab) == 0) break

        xvocab <- xvocab[xvocab$doc_count == max(xvocab$doc_count),]
        if (nrow(xvocab) == 0) break

        xvocab <- xvocab[1,]

        xvectorizer <- vocab_vectorizer(xvocab)
        xdtm <- as.data.frame(as.matrix(create_dtm(xit, xvectorizer)))
        colnames(xdtm) <- 'target'

        dups <- rownames(xdtm %>% filter(target > 0))
        dups <- dups[2:length(dups)]

        result <- filter(result, !(job_id %in% dups))
    }

    result
}


# noah resume work
cossim_noah_tibble <- cossim_noah %>% 
  rownames_to_column(var = "job_id") %>% 
  as_tibble() %>% 
  rename(cos_simil = `RESUME (NOAH)`) %>% 
  arrange(cos_simil, ascending =TRUE)  %>% 
  merge(full_df)

cossim_noah_tibble <- cossim_noah_tibble %>% 
  distinct(job_desc, .keep_all = TRUE)

top_per_category_noah <- cossim_noah_tibble %>% 
  group_by(job_title) %>% 
  top_n(1, cos_simil)

top_per_category_noah <- prune_jobs_with_identical_sentences(top_per_category_noah)

top_per_category_noah

top_3_jobs <- top_per_category_noah %>% 
  ungroup() %>% 
  top_n(3, cos_simil) %>% 
  select(job_title, job_desc, job_location, cos_simil)


bottom_one <- top_per_category_noah %>% 
  ungroup() %>% 
  top_n(-1, cos_simil) %>% 
  select(job_title, job_desc, job_location, cos_simil)


```




### Data issues and creating additional diversity
One of the largest issues with getting a diverse selection of jobs is preventing the same company from being chosen multiple times. For example, many companies even cross posted the exact same job into two separate categories. Or they would post the exact same job, but have a different ID because they were hoping to fill the job in California and New York separately. 

To fix these issues, we found there were two fixes. First, duplicate entries, with the exact same (word-for-word) job description were deleted. Second, we used n-grams to delete jobs from the same company. We noticed that companies would post a brief description about the company in the job description, so we could filter through them using duplicate n-grams. If two job descriptions had 8 words in a row duplicated with another job description, one of them was pruned from our data. This prevented Ido for example from recieving 3 Palo Alto job recommendations. 


### Ensuring diverse jobs
Our way of ensuring diverse jobs and provide interesting choices was to use the job categories provided by indeed. Very few jobs were extremely similar across categories, as there aren't a lot of similar words in a marketing job as an actuary. So after finding the top job in each category, we selected the top across those 25 categories. As a result, we found that for various strings, there were often two somewhat similar category jobs but then one much more diverse. As a result, it is likely you miss out on some more fitted jobs in the name of a more diverse selection but this makes job hunting a lot more fun. 

However, just by choosing top by category doesn't mean we will be similar to that category. In fact, even the "best" job for someone in the category might still be really bad. For example, Noah's "worst" of all the categories can be seen below, a full stack engineer (for Vox), a position he is not well equipped for and decently far from his chosen input. 

```{r}
bottom_one
```



However, the top three look very promising, good and diverse:


```{r echo=FALSE}

top_3_jobs

```
As an example of a good match, here is one of my top results.

```{r echo=FALSE}
example_good <- head(top_3_jobs %>% pull(2,1), 1)

str_trunc(example_good, 500, "right")
```



```{r echo=FALSE}
# Ido results

cossim_ido_tibble <- cossim_ido %>% 
  rownames_to_column(var = "job_id") %>% 
  as_tibble() %>% 
  rename(cos_simil = `RESUME (IDO)`) %>% 
  arrange(cos_simil, ascending =TRUE)  %>% 
  merge(full_df)

# remove duplicates

cossim_ido_tibble <- cossim_ido_tibble %>% 
  distinct(job_desc, .keep_all = TRUE)

top_per_category_ido <- cossim_ido_tibble %>% 
  group_by(job_title) %>% 
  top_n(1, cos_simil)

top_per_category_ido <- prune_jobs_with_identical_sentences(top_per_category_ido)

# top_per_category_ido

top_3_jobs <- top_per_category_ido %>% 
  ungroup() %>% 
  top_n(3, cos_simil) %>% 
  select(job_title, job_desc, job_location)

top_3_jobs

```




### Measuring Diversity
To measure diversity, we decided to look at the similarity between our final three choices. If these were high, we would know that they were unlikely diverse. As such, we created a cosine similarly between the final three choices and took the average up the upper corner to get a "diversity score" of our final selection. The closer to 0 would represent more diverse, whereas closer to 1 means they are very similar. For example, here is the similarity matrix for Ido's results as well as his score.

```{r echo=FALSE, message=FALSE, warning=FALSE}
it <- itoken(
  top_3_jobs$job_desc, # where to tokenize
  preprocessor = tolower, # How to preprocess
  tokenizer = word_tokenizer, # what tokenizer
  ids = top_3_jobs$job_id, # keep id as identifier
  progressbar = FALSE
)

stop_words <- c("a","an","and","are","as","at","be","by","for","from","have","in","is","i","not","of","on","or","other","our","ours","that","the","their","this","to","we","who","with","you","your","yours","me","my","myself","ourselves","")

vocab <- create_vocabulary(
  it,
  stopwords = stop_words,
  ngram = c(1L, 3L)
)


vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it, vectorizer)
# dim(dtm)

tfidf <- TfIdf$new()
tfidf_dtm <- fit_transform(dtm, tfidf)

cossim <- as.data.frame(as.matrix(sim2(
  x = tfidf_dtm,
  method = "cosine",
  norm = "l2"
)))

cossim

div_score <- sum(cossim[1,2],cossim[1,3],cossim[2,3])/3


cat("Div. Score:" , div_score)
```
