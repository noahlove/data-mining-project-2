---
title: "Project 2 - Job Hunt"
author: "Noah Love and Ido Li On"
date: "3/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, message=FALSE, warning=FALSE, include=FALSE}

library(tidyverse)
library(tidyjson)
library(jsonlite)
library(text2vec)
library(data.table)
```


## Introduction

The data provided to us for this report is a scraped job postings from [indeed.com](https://www.indeed.com/ "https://www.indeed.com/"). They were scraped from September 2020 to March 2021, in four distinct scrapes.

## The data

So the data is presented in a json file. It is a list of 16 (or 20 for 2021) lists, each representing a unique job category: i.e. ux designer, recruiter or marketing. Within each of the 16 lists, are 2 lists: the first is a list of 4, describing position, location, job time (full or part time), and start time. The second list contains job descriptions associated with that job category.

### Clean up the data

Our first major choice was to combine all four data sets (different dates) into one large csv. None of our questions necessarily related to the time aspect of the data so we chose to just unify them. Here is an example of the list of categories, and associated characteristics.

```{r echo=FALSE}
full_df <- as_tibble(read.csv("Indeed_data/merged.csv"))

job_positions <- full_df %>% 
  group_by(job_title) %>% 
  summarise(n = n())

job_positions

```

First, there are only 25 categories here. Definitely not a comprehensive view of the job market, so if you are really set about going into a field that is not listed, this loop project explained below would not be a good fit. However, for most people majoring in statistics or computer science, this might provide a nice survey of various jobs. 

Another question that would be reasonable to ask is what is the distribution of job postings per category?

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data = job_positions, aes(x = job_title, y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  ggtitle("Distribution of the jobs by category for 2020") +
  coord_flip() +
  theme_light()+
  labs(x = "Job Title", y = "Number of jobs in the category")

```

Hopefully you don't want to be a data journalist because the data is not as comprehensive than the rest. But otherwise, it seems to be very equal across the categories. Otherwise, for the categories mentioned, it seems that the data is representative and could be very useful in a job hunt if you have interests in the related field. 

```{r echo=FALSE}
full_df %>% 
  group_by(job_location) %>% 
  summarise(n = n()) %>% 
  ggplot(aes(x = job_location, y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  ggtitle("Where are the jobs?") +
  labs(x = "State", y = "Number of jobs")
```
The data is likely most lacking in location. For people that were hoping for rural jobs, or maybe remote jobs, the data scraped is specifically focused on California and New York State which is not for everyone. 

## Wrangle the data

We wrangled all of these into a csv format for easier working. The data frame gives job location, salary minimum and maximum (rare), separate columns for different types of job positions (full time, part time, contract or internship) as well as job title (category), job_id (distinct id number from Indeed) and the job description from the company.

```{r message=FALSE, warning=FALSE, include=FALSE}
glimpse(full_df)
```

Here is an (shortened) example of how each job postings appears in JSON:

> "2ccc2596d13c54d4": "Job to be done\nWe\u2019re the best way to get better.\n\nAbout Buoy Health:\nBuoy builds a digital health tool that helps people \u2013 from the moment they get sick \u2013 start their health care on the right foot. Started by a team of doctors and computer scientists working at the Harvard Innovation Laboratory in Boston MA, Buoy was developed in direct response to the downward spiral we\u2019ve all faced when we attempt to self-diagnose our symptoms online...."

### Data Quality Issue

Some companies appear to have gotten lazy in their copy and pasting between websites and have smashed some words together. Because it is a rare enough occurrence, it will hopefully not have a large effect on the data. 

## Data processing

The data was then tokenized using the package text2vec. Once the job descriptions were tokenized, then we could proceed to create a cosine similarity matrix for the job postings, identified by their Indeed ID. Cosine similiarity is a measure of similarity between two vectors, in this case two job postings. The values range from 0 to 1 (the same posting). This method was chosen because of the type of data. Instead of having to come up with a clever data normalization technique due to some companies being incredibly verbose while others are super brief, this takes care of that instead.

```{r cache=FALSE, include=FALSE}
cosine_matrix <- as_tibble(read.csv("Indeed_data/cos_dissm_mat.csv.gz"))

cosine_matrix[1:4, 1:4]
```

## User Input
Now, we will ask for user input, in a somewhat manual way. Based on your resume, we choose keywords from previous job positions, courses and trainings that you think would be relevant to your job search.

How does this help? We will turn the words that are entered from your resume into a unique tokenized string as well. This will be put into the cosine similarity matrix, and we can compare which job postings you have the most in common with. Using the post ID, we can then filter the job postings such that you can get 3 recommendations from 3 distinct Indeed job categories to give you a variety of options to choose from.

```{r}
noah_input <- "Coins Gold Bullion Banking Diamonds Website Development Software Coding Programming Hedge Investment Fund Social Media Student Salesman Entrepreneurship, Startup, Incubator, HTML, R, Python"

ido_input <-  "Programming Cybersecurity Networks Reverse Engineering"
```

```{r include=FALSE}
# Add user to csv, should need "description" and "job id"
user_full <- full_df %>%
  add_row(job_id = "RESUME (NOAH)", job_desc = noah_input, job_location = "A") %>%
  add_row(job_id = "RESUME (IDO)", job_desc = ido_input, job_location = "A") %>%
  arrange(job_location)

# head(user_full, 4) # make sure user was added
```


```{r include=FALSE}
it <- itoken(
  user_full$job_desc, # where to tokenize
  preprocessor = tolower, # How to preprocess
  tokenizer = word_tokenizer, # what tokenizer
  ids = user_full$job_id, # keep id as identifier
  progressbar = FALSE
)

stop_words <- c("a","an","and","are","as","at","be","by","for","from","have","in","is","i","not","of","on","or","other","our","ours","that","the","their","this","to","we","who","with","you","your","yours","me","my","myself","ourselves","")

vocab <- create_vocabulary(
  it,
  stopwords = stop_words,
  ngram = c(1L, 3L)
)

# tail(vocab, 5)
```
```{r include=FALSE}
vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it, vectorizer)
# dim(dtm)
```

```{r include=FALSE}
tfidf <- TfIdf$new()
tfidf_dtm <- fit_transform(dtm, tfidf)

cossim <- as.data.frame(as.matrix(sim2(
  x = tfidf_dtm,
  method = "cosine",
  norm = "l2"
)))
```

Here is an example of the cosine similarity matrix. Noah's resume is the same as itself so it has a similarity of 1. It is also interestly almost nothing like Ido's (very close to 0). 


```{r include=FALSE}
cossim_noah <- cossim %>% select(1)
cossim_ido <- cossim %>% select(2)

head(cossim_noah, 10)
```

### Data issues and creating additional diversity
One of the largest issues with getting a diverse selection of jobs is preventing the same company from being chosen multiple times. For example, many companies even cross posted the exact same job into two separate categories. Or they would post the exact same job, but have a different ID because they were hoping to fill the job in California and New York separately. 

To fix these issues, we found there were two fixes. First, duplicate entries, with the exact same (word-for-word) job description were deleted. Second, we used n-grams to delete jobs from the same company. We noticed that companies would post a brief description about the company in the job description, so we could filter through them using duplicate n-grams. If two job descriptions had 8 words in a row duplicated with another job description, one of them was pruned from our data. This prevented Ido for example from recieving 3 Palo Alto job recommendations. 

```{r include=FALSE}
prune_jobs_with_identical_sentences <- function(data, word_count = 8)
{
    result <- data

    while (1)
    {
        xit <- itoken(
          result$job_desc, # where to tokenize
          preprocessor = tolower, # How to preprocess
          tokenizer = word_tokenizer, # what tokenizer
          ids = result$job_id, # keep id as identifier
          progressbar = FALSE
        )

        xvocab <- create_vocabulary(xit, ngram = c(word_count,word_count))

        xvocab <- prune_vocabulary(xvocab, doc_count_min = 2)
        if (nrow(xvocab) == 0) break

        xvocab <- xvocab[xvocab$doc_count == max(xvocab$doc_count),]
        if (nrow(xvocab) == 0) break

        xvocab <- xvocab[1,]

        xvectorizer <- vocab_vectorizer(xvocab)
        xdtm <- as.data.frame(as.matrix(create_dtm(xit, xvectorizer)))
        colnames(xdtm) <- 'target'

        dups <- rownames(xdtm %>% filter(target > 0))
        dups <- dups[2:length(dups)]

        result <- filter(result, !(job_id %in% dups))
    }

    result
}


# noah resume work
cossim_noah_tibble <- cossim_noah %>% 
  rownames_to_column(var = "job_id") %>% 
  as_tibble() %>% 
  rename(cos_simil = `RESUME (NOAH)`) %>% 
  arrange(cos_simil, ascending =TRUE)  %>% 
  merge(full_df)

cossim_noah_tibble <- cossim_noah_tibble %>% 
  distinct(job_desc, .keep_all = TRUE)

top_per_category_noah <- cossim_noah_tibble %>% 
  group_by(job_title) %>% 
  top_n(1, cos_simil)

top_per_category_noah <- prune_jobs_with_identical_sentences(top_per_category_noah)

# top_per_category_noah

top_3_jobs <- top_per_category_noah %>% 
  ungroup() %>% 
  top_n(3, cos_simil) %>% 
  select(job_title, job_desc, job_location)

top_3_jobs


```

```{r include=FALSE}
# Ido results

cossim_ido_tibble <- cossim_ido %>% 
  rownames_to_column(var = "job_id") %>% 
  as_tibble() %>% 
  rename(cos_simil = `RESUME (IDO)`) %>% 
  arrange(cos_simil, ascending =TRUE)  %>% 
  merge(full_df)

# remove duplicates

cossim_ido_tibble <- cossim_ido_tibble %>% 
  distinct(job_desc, .keep_all = TRUE)

top_per_category_ido <- cossim_ido_tibble %>% 
  group_by(job_title) %>% 
  top_n(1, cos_simil)

top_per_category_ido <- prune_jobs_with_identical_sentences(top_per_category_ido)

# top_per_category_ido

top_3_jobs <- top_per_category_ido %>% 
  ungroup() %>% 
  top_n(3, cos_simil) %>% 
  select(job_title, job_desc, job_location)

top_3_jobs

```



